# Clickstream Analytics Pipeline Using Apache Spark & Delta Lake
This repository contains a complete end-to-end big data analytics pipeline built to process, optimize, and analyze large-scale clickstream data originating from a real e-commerce platform. The entire development process was conducted using Databricks, and all included .ipynb notebooks are structured so they can be opened and executed directly within a Databricks workspace. The project uses Apache Spark and Delta Lake to deliver a scalable, fault-tolerant processing environment capable of supporting both batch and near real-time analytical workloads.

The data used in this project was obtained from Kaggle through the following public dataset:
https://www.kaggle.com/datasets/mkechinov/ecommerce-behavior-data-from-multi-category-store/data
.
The dataset includes several months of clickstream logs; however, this project focuses specifically on the 2019-Oct.csv file, which alone contains more than 42 million user interaction events. Due to the datasetâ€™s very large size, the raw CSV file was first ingested and then transformed into Parquet format. This conversion significantly improved the efficiency of data loading, scanning, and processing within Spark and Databricks, enabling the subsequent ETL, modeling, and analytical tasks to run more smoothly and cost-effectively.

The pipeline follows the Medallion Architecture, progressing from raw ingestion into Bronze, Silver, and ultimately Gold data layers. Each layer implements structured transformations, data cleaning procedures, schema enforcement, and the creation of business-ready fact and dimension tables. The project places significant emphasis on addressing distributed processing challenges. It includes detailed implementations of major Spark optimizations such as temporary SQL view creation to reduce repeated I/O, broadcast joins to eliminate expensive shuffles when working with small dimension tables, repartitioning strategies for efficient session-level computations, salting techniques to mitigate severe event-type data skew, and predicate pushdown to drastically reduce storage reads. For each optimization, the repository provides the technical reasoning, implementation approach, and quantitative performance improvement.

Beyond performance engineering, the project integrates key data governance and reliability features supported by Delta Lake. These include ACID transactional writes, schema enforcement with automatic validation of incoming data, and version-controlled time travel capabilities for safe rollback, auditing, and reproducibility. Throughout the pipeline, detailed data quality assessments ensure that the processed data remains consistent and analytically trustworthy.

The final analytical output is presented through a multi-page dashboard designed around a coherent data storytelling methodology. The dashboard progresses from high-level executive KPIs to deeper analyses of conversion funnels, product performance, user segmentation, and temporal behavioral trends. It also visualizes the improvements achieved through the implemented Spark optimizations, allowing both technical and business stakeholders to clearly understand the impact of engineering decisions on performance, scalability, and cost.

The repository provides an organized project structure that enables full reproducibility. It contains the full project report, the Databricks-ready .ipynb notebooks, ETL and optimization scripts, and exported dashboard results. Users can replicate the ingestion, transformation, and analytical steps directly within their own Databricks environment using the same dataset.

This project was completed by Amr Ahmed, Marwan Ahmed, and Mohamed Ahmed as part of the Big Data Analytics (CIE 427) course within the Communications and Information Engineering Program at Zewail City of Science, Technology and Innovation during Fall 2025.
